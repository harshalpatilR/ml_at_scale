{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"Airline ML\")\\\n",
    "    .config(\"spark.executor.memory\",\"16g\")\\\n",
    "    .config(\"spark.executor.cores\",\"4\")\\\n",
    "    .config(\"spark.driver.memory\",\"6g\")\\\n",
    "    .config(\"spark.executor.instances\",\"5\")\\\n",
    "    .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\",\"org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider\")\\\n",
    "    .config(\"spark.yarn.access.hadoopFileSystems\",\"s3a://ml-field\")\\\n",
    "    .config(\"spark.dynamicAllocation.enabled\",\"false\")\\    \n",
    ".getOrCreate()\n",
    "\n",
    "flight_df=spark.read.parquet(\n",
    "  \"s3a://ml-field/demo/flight-analysis/data/airline_parquet_2/\",\n",
    ")\n",
    "\n",
    "flight_df = flight_df.na.drop() #.limit(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"http://spark-z9e8kflj0nbe3di0.ml-41c57ba2-8b4.ml-prod.vnu8-sqze.cloudera.site\">Spark UI</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "import os\n",
    "HTML('<a href=\"http://spark-{}.{}\">Spark UI</a>'.format(os.getenv(\"CDSW_ENGINE_ID\"),os.getenv(\"CDSW_DOMAIN\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- FL_DATE: timestamp (nullable = true)\n",
      " |-- OP_CARRIER: string (nullable = true)\n",
      " |-- OP_CARRIER_FL_NUM: string (nullable = true)\n",
      " |-- ORIGIN: string (nullable = true)\n",
      " |-- DEST: string (nullable = true)\n",
      " |-- CRS_DEP_TIME: string (nullable = true)\n",
      " |-- CRS_ARR_TIME: string (nullable = true)\n",
      " |-- CANCELLED: double (nullable = true)\n",
      " |-- CRS_ELAPSED_TIME: double (nullable = true)\n",
      " |-- DISTANCE: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flight_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import udf,substring,weekofyear,concat,col,when,length,lit\n",
    "\n",
    "#No longer needed\n",
    "#convert_time_to_hour = udf(lambda x: x if len(x) == 4 else \"0{}\".format(x),StringType())\n",
    "\n",
    "flight_df\\\n",
    "    .withColumn(\n",
    "        'CRS_DEP_HOUR',\n",
    "        when(\n",
    "            length(col(\"CRS_DEP_TIME\")) == 4,col(\"CRS_DEP_TIME\")\n",
    "        )\\\n",
    "        .otherwise(concat(lit(\"0\"),col(\"CRS_DEP_TIME\")))\n",
    "    )\\\n",
    "    .select(\"CRS_DEP_HOUR\")\\\n",
    "flight_df = flight_df.withColumn('CRS_DEP_HOUR',col('CRS_DEP_HOUR').cast('double'))\n",
    "flight_df = flight_df.withColumn('WEEK',weekofyear('FL_DATE').cast('double'))\n",
    "flight_df = flight_df.withColumn(\"ROUTE\", concat(col(\"ORIGIN\"),col(\"DEST\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- FL_DATE: timestamp (nullable = true)\n",
      " |-- OP_CARRIER: string (nullable = true)\n",
      " |-- OP_CARRIER_FL_NUM: string (nullable = true)\n",
      " |-- ORIGIN: string (nullable = true)\n",
      " |-- DEST: string (nullable = true)\n",
      " |-- CRS_DEP_TIME: string (nullable = true)\n",
      " |-- CRS_ARR_TIME: string (nullable = true)\n",
      " |-- CANCELLED: double (nullable = true)\n",
      " |-- CRS_ELAPSED_TIME: double (nullable = true)\n",
      " |-- DISTANCE: double (nullable = true)\n",
      " |-- CRS_DEP_HOUR: double (nullable = true)\n",
      " |-- WEEK: double (nullable = true)\n",
      " |-- ROUTE: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flight_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "\n",
    "numeric_cols = [\"CRS_ELAPSED_TIME\",\"DISTANCE\",\"WEEK\",\"CRS_DEP_HOUR\"]\n",
    "\n",
    "op_carrier_indexer = StringIndexer(inputCol ='OP_CARRIER', outputCol = 'OP_CARRIER_INDEXED',handleInvalid=\"keep\")\n",
    "op_carrier_encoder = OneHotEncoder(inputCol ='OP_CARRIER_INDEXED', outputCol='OP_CARRIER_ENCODED')\n",
    "\n",
    "origin_indexer = StringIndexer(inputCol ='ORIGIN', outputCol = 'ORIGIN_INDEXED',handleInvalid=\"keep\")\n",
    "origin_encoder = OneHotEncoder(inputCol ='ORIGIN_INDEXED', outputCol='ORIGIN_ENCODED')\n",
    "\n",
    "dest_indexer = StringIndexer(inputCol ='DEST', outputCol = 'DEST_INDEXED',handleInvalid=\"keep\")\n",
    "dest_encoder = OneHotEncoder(inputCol ='DEST_INDEXED', outputCol='DEST_ENCODED')\n",
    "\n",
    "route_indexer = StringIndexer(inputCol ='ROUTE', outputCol = 'ROUTE_INDEXED',handleInvalid=\"keep\")\n",
    "route_encoder = OneHotEncoder(inputCol ='ROUTE_INDEXED', outputCol='ROUTE_ENCODED')\n",
    "\n",
    "input_cols=[\n",
    "    'OP_CARRIER_ENCODED',\n",
    "    #'ROUTE_INDEXED',\n",
    "    'ORIGIN_ENCODED',\n",
    "    'DEST_ENCODED'] + numeric_cols\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols = input_cols,\n",
    "    outputCol = 'features')\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipeline_indexed_only = Pipeline(\n",
    "    stages=[\n",
    "        op_carrier_indexer,\n",
    "        op_carrier_encoder,\n",
    "        origin_indexer,\n",
    "        origin_encoder,\n",
    "        dest_indexer,\n",
    "        dest_encoder,\n",
    "        #route_indexer,\n",
    "        assembler]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CANCELLED: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipelineModel = pipeline_indexed_only.fit(flight_df)\n",
    "model_df = pipelineModel.transform(flight_df)\n",
    "selectedCols = ['CANCELLED', 'features']# + cols\n",
    "model_df = model_df.select(selectedCols)\n",
    "model_df.printSchema()\n",
    "(train, test) = model_df.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(featuresCol = 'features', labelCol = 'CANCELLED', maxIter=10)\n",
    "\n",
    "lrModel = lr.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aggregationDepth: suggested depth for treeAggregate (>= 2) (default: 2)\n",
      "elasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty (default: 0.0)\n",
      "family: The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial. (default: auto)\n",
      "featuresCol: features column name (default: features, current: features)\n",
      "fitIntercept: whether to fit an intercept term (default: True)\n",
      "labelCol: label column name (default: label, current: CANCELLED)\n",
      "lowerBoundsOnCoefficients: The lower bounds on coefficients if fitting under bound constrained optimization. (undefined)\n",
      "lowerBoundsOnIntercepts: The lower bounds on intercepts if fitting under bound constrained optimization. (undefined)\n",
      "maxIter: maximum number of iterations (>= 0) (default: 100, current: 10)\n",
      "predictionCol: prediction column name (default: prediction)\n",
      "probabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities (default: probability)\n",
      "rawPredictionCol: raw prediction (a.k.a. confidence) column name (default: rawPrediction)\n",
      "regParam: regularization parameter (>= 0) (default: 0.0)\n",
      "standardization: whether to standardize the training features before fitting the model (default: True)\n",
      "threshold: threshold in binary classification prediction, in range [0, 1] (default: 0.5)\n",
      "thresholds: Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0 excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class's threshold (undefined)\n",
      "tol: the convergence tolerance for iterative algorithms (>= 0) (default: 1e-06)\n",
      "upperBoundsOnCoefficients: The upper bounds on coefficients if fitting under bound constrained optimization. (undefined)\n",
      "upperBoundsOnIntercepts: The upper bounds on intercepts if fitting under bound constrained optimization. (undefined)\n",
      "weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0 (undefined)\n"
     ]
    }
   ],
   "source": [
    "print(lrModel.explainParams())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.712283740399597"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "predictionslr = lrModel.transform(test)\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"CANCELLED\",metricName=\"areaUnderROC\")\n",
    "evaluator.evaluate(predictionslr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set areaUnderROC: 0.7118916290251833\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "trainingSummary = lrModel.summary\n",
    "roc = trainingSummary.roc.toPandas()\n",
    "plt.plot(roc['FPR'],roc['TPR'])\n",
    "plt.ylabel('False Positive Rate')\n",
    "plt.xlabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.show()\n",
    "#print('Training set areaUnderROC: ' + str(trainingSummary.areaUnderROC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CANCELLED: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_cols=[\n",
    "    'OP_CARRIER_INDEXED',\n",
    "    #'ROUTE_INDEXED',\n",
    "    'ORIGIN_INDEXED',\n",
    "    'DEST_INDEXED'] + numeric_cols\n",
    "\n",
    "pipelineModel = pipeline_indexed_only.fit(flight_df)\n",
    "model_df = pipelineModel.transform(flight_df)\n",
    "selectedCols = ['CANCELLED', 'features']# + cols\n",
    "model_df = model_df.select(selectedCols)\n",
    "model_df.printSchema()\n",
    "(train, test) = model_df.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "rfclassifier = RandomForestClassifier(labelCol = 'CANCELLED', featuresCol = 'features', maxBins=390)\n",
    "rfmodel = rfclassifier.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "predictionsrf = rfmodel.transform(test)\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"CANCELLED\",metricName=\"areaUnderROC\")\n",
    "evaluator.evaluate(predictionsrf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import GBTClassifier\n",
    "gbt = GBTClassifier(maxIter=10,featuresCol = 'features', labelCol = 'CANCELLED')\n",
    "\n",
    "gbtModel = gbt.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictionsgbt = gbtModel.transform(test)\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"CANCELLED\",metricName=\"areaUnderROC\")\n",
    "evaluator.evaluate(predictionsgbt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
